{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership inference attack with images\n",
    "## Target a CNN\n",
    "Authors : Johan Jublanc\n",
    "\n",
    "We use this article to simulate a membership inference attack : https://arxiv.org/pdf/1807.09173.pdf\n",
    "\n",
    "Usefull reference : https://medium.com/disaitek/demystifying-the-membership-inference-attack-e33e510a0c39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# tensorflow privacy\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPAdamGaussianOptimizer\n",
    "\n",
    "# tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# sklearn and xgb\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.__version__ == 2.x\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the CIFAR10 data which is a dataset of color images of size 32x32. For more information let's go here :\n",
    "- https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "CIFAR10 data are splited in batches. For this example the first batche is used to build up a classifier and the second one will be used to build up the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def get_data():\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    data_dir = tf.keras.utils.get_file(origin=url, fname='cifar10', untar=True)\n",
    "\n",
    "    root_keras_data_path = \"/\".join(data_dir.split(\"/\")[:5])\n",
    "    cifar_data_path = os.path.join(root_keras_data_path, \"cifar-10-batches-py\")\n",
    "\n",
    "    data_batches_names = []\n",
    "    for item in os.listdir(cifar_data_path):\n",
    "        if item.startswith(\"data_batch\"):\n",
    "            data_batches_names.append(item)\n",
    "    \n",
    "    print(\"Files used to build the data list : \")\n",
    "    \n",
    "    data = []\n",
    "    for data_batches_name in data_batches_names:\n",
    "        data.append(unpickle(os.path.join(cifar_data_path, data_batches_name)))\n",
    "        print(data_batches_name)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = get_data()\n",
    "\n",
    "def process_data(data):\n",
    "    flat_array_normalized = data / 255.\n",
    "    img_reshaped = np.reshape(flat_array_normalized, (10000, 3, 32, 32))\n",
    "    data = np.transpose(img_reshaped, (0, 2, 3, 1))\n",
    "    return data\n",
    "\n",
    "#############################\n",
    "# Data for the target model #\n",
    "#############################\n",
    "\n",
    "x_a = data[0][b\"data\"]\n",
    "y_a = data[0][b\"labels\"] #[sparse_labels(x) for x in data[0][b\"labels\"]]\n",
    "\n",
    "x_b = data[1][b\"data\"]\n",
    "y_b = data[1][b\"labels\"] #[sparse_labels(x) for x in data[1][b\"labels\"]]\n",
    "\n",
    "x_a = process_data(x_a)\n",
    "x_b = process_data(x_b)\n",
    "\n",
    "y_a = tf.keras.utils.to_categorical(y_a, num_classes=10)\n",
    "y_b = tf.keras.utils.to_categorical(y_b, num_classes=10)\n",
    "\n",
    "#######################\n",
    "# Data for the attack #\n",
    "#######################\n",
    "\n",
    "x_prim_in = data[3][b\"data\"]\n",
    "y_prim_in = data[3][b\"labels\"]\n",
    "\n",
    "x_prim_out = data[4][b'data']\n",
    "y_prim_out = data[4][b\"labels\"]\n",
    "\n",
    "x_prim_in = process_data(x_prim_in)\n",
    "x_prim_out = process_data(x_prim_out)\n",
    "\n",
    "y_prim_in = tf.keras.utils.to_categorical(y_prim_in, num_classes=10)\n",
    "y_prim_out = tf.keras.utils.to_categorical(y_prim_out, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "batch_size = 250\n",
    "\n",
    "input_shape = x_a[0].shape\n",
    "\n",
    "l2_norm_clip = 1.5\n",
    "noise_multiplier = 0 #1.5\n",
    "num_microbatches = 250\n",
    "learning_rate = 0.001\n",
    "\n",
    "if batch_size % num_microbatches != 0:\n",
    "  raise ValueError('Batch size should be an integer multiple of the number of microbatches')\n",
    "\n",
    "dropout = True\n",
    "\n",
    "train_target = True\n",
    "if noise_multiplier > 0:\n",
    "    train_shadow = False\n",
    "else:\n",
    "    train_shadow = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name_patterns(dropout, noise_multiplier):\n",
    "    pattern_target = \"models/target\" + dropout*\"WithDropOut\" + \"DP\" + str(noise_multiplier) + \"_*.h5\"\n",
    "    pattern_shadow = \"models/shadow\" + dropout*\"WithDropOut\" + \"_*.h5\"\n",
    "    pattern_graphs_loss = \"graphs/graphLoss\" + dropout*\"WithDropOut\" + \"DP\" + str(noise_multiplier) + \".png\"\n",
    "    pattern_graphs_accuracy = \"graphs/graphAccuracy\" + dropout*\"WithDropOut\" + \"DP\" + str(noise_multiplier) + \".png\"\n",
    "    print(pattern_target)\n",
    "    print(pattern_shadow)\n",
    "    print(pattern_graphs_loss)\n",
    "    print(pattern_graphs_accuracy)\n",
    "    return pattern_target, pattern_shadow, pattern_graphs_loss, pattern_graphs_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pattern_target, pattern_shadow, pattern_graphs_loss, pattern_graphs_accuracy = get_model_name_patterns(dropout, noise_multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_labels(x, y):\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        #img = rotate(img_batch[n], -90)\n",
    "        plt.imshow(x[n])\n",
    "        plt.title(np.argmax(y[n]))\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_img_labels(x_a, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    if dropout:\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    if dropout:\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load/Save model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"models\" not in os.listdir():\n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_model_num(pattern):\n",
    "    target_models_list = glob.glob(pattern)\n",
    "    num_list = [x.split(\"/\")[1].split(\".\")[-2].split(\"_\")[1] for x in target_models_list]\n",
    "    num_list_int = [int(x) for x in num_list]\n",
    "    return np.max(num_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_model(pattern, model):\n",
    "    if len(glob.glob(pattern))==0:\n",
    "        model.save(pattern.split(\"_\")[0] + \"_0.h5\")\n",
    "    else:\n",
    "        num = return_model_num(pattern) + 1\n",
    "        model.save(pattern.replace(\"*\", str(num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(noise_multiplier, \n",
    "                  l2_norm_clip,\n",
    "                  num_microbatches,\n",
    "                  learning_rate):\n",
    "    if noise_multiplier > 0 :\n",
    "        optimizer = DPAdamGaussianOptimizer(l2_norm_clip=l2_norm_clip,\n",
    "                                            noise_multiplier=noise_multiplier,\n",
    "                                            num_microbatches=num_microbatches,\n",
    "                                            learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_compile(model, optimizer):\n",
    "    print(\"Manual compilation of the model\")\n",
    "    print(\"Optimizer : {}\".format(optimizer))\n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction=tf.losses.Reduction.NONE),\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_model(input_shape, \n",
    "                  noise_multiplier, \n",
    "                  l2_norm_clip,\n",
    "                  num_microbatches,\n",
    "                  learning_rate,\n",
    "                  dropout):\n",
    "    \n",
    "    model = create_model(input_shape, dropout)\n",
    "    optimizer = get_optimizer(noise_multiplier, \n",
    "                              l2_norm_clip,\n",
    "                              num_microbatches,\n",
    "                              learning_rate)\n",
    "    model = model_compile(model, optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(pattern, \n",
    "               noise_multiplier, \n",
    "               l2_norm_clip,\n",
    "               num_microbatches,\n",
    "               learning_rate):\n",
    "    \n",
    "    num = return_model_num(pattern)\n",
    "    filepath = pattern.replace(\"*\", str(num))\n",
    "    \n",
    "    model = tf.keras.models.load_model(filepath)\n",
    "    print(\"\\nModel retrieved from the file : {}\\n\".format(filepath))\n",
    "    optimizer = get_optimizer(noise_multiplier, \n",
    "                              l2_norm_clip,\n",
    "                              num_microbatches,\n",
    "                              learning_rate)\n",
    "    model = model_compile(model, optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train target model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load pretrained or create a new model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(data_train,\n",
    "                         data_test,\n",
    "                         num_epochs,\n",
    "                         batch_size,\n",
    "                         pattern, \n",
    "                         input_shape, \n",
    "                         noise_multiplier, \n",
    "                         l2_norm_clip,\n",
    "                         num_microbatches,\n",
    "                         learning_rate,\n",
    "                         dropout\n",
    "                        ):\n",
    "    \n",
    "    ##################\n",
    "    #create the model#\n",
    "    ##################\n",
    "    model = get_new_model(input_shape, noise_multiplier, \n",
    "                                       l2_norm_clip,\n",
    "                                       num_microbatches,\n",
    "                                       learning_rate,\n",
    "                                       dropout)\n",
    "    score = model.evaluate(data_test[0][:1000], data_test[1][:1000], batch_size=500, verbose=0)\n",
    "\n",
    "    print(\"\\n\\nPERFORMANCES BEFORE TRAINNING: \")\n",
    "    print('Test loss:', np.mean(score[0]))\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    #######\n",
    "    #train#\n",
    "    #######\n",
    "    history=None\n",
    "    history = model.fit(data_train[0], data_train[1],\n",
    "                        epochs=num_epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=1,\n",
    "                        validation_data=data_test)\n",
    "\n",
    "    ######\n",
    "    #save#\n",
    "    ######\n",
    "    save_new_model(pattern, model)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_target:\n",
    "    history_target = train_and_save_model((x_b,  y_b),\n",
    "                                          (x_a, y_a),\n",
    "                                          epochs,\n",
    "                                          batch_size,\n",
    "                                          pattern_target, \n",
    "                                          input_shape, \n",
    "                                          noise_multiplier, \n",
    "                                          l2_norm_clip,\n",
    "                                          num_microbatches,\n",
    "                                          learning_rate,\n",
    "                                          dropout\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = load_model(pattern_target,\n",
    "                          noise_multiplier, \n",
    "                          l2_norm_clip,\n",
    "                          num_microbatches,\n",
    "                          learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = target_model.evaluate(x_a, y_a, batch_size=500, verbose=0)\n",
    "    \n",
    "print(\"\\n\\nPERFORMANCES AFTER TRAINNING: \")\n",
    "print('Test loss:', np.mean(score[0]))\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the shadow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_shadow:\n",
    "    history_shadow = train_and_save_model((x_prim_in,  y_prim_in),\n",
    "                                          (x_prim_out, y_prim_out),\n",
    "                                          epochs,\n",
    "                                          batch_size,\n",
    "                                          pattern_shadow, \n",
    "                                          input_shape, \n",
    "                                          noise_multiplier=0, \n",
    "                                          l2_norm_clip=l2_norm_clip,\n",
    "                                          num_microbatches=num_microbatches,\n",
    "                                          learning_rate=learning_rate,\n",
    "                                          dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shadow = load_model(pattern_shadow, \n",
    "                          noise_multiplier=0,\n",
    "                          l2_norm_clip=l2_norm_clip,\n",
    "                          num_microbatches=num_microbatches,\n",
    "                          learning_rate=learning_rate)\n",
    "model_shadow.summary()\n",
    "score = model_shadow.evaluate(x_prim_out, y_prim_out, batch_size=500, verbose=0)\n",
    "\n",
    "print(\"\\n\\nPERFORMANCES BEFORE TRAINING: \")\n",
    "print('Test loss:', np.mean(score[0]))\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"graphs\" not in os.listdir():\n",
    "    os.mkdir(\"graphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reduce_loss(val_loss):\n",
    "    val_result = []\n",
    "    for loss in val_loss:\n",
    "        val_result.append(np.mean(loss))\n",
    "    return val_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_shadow:\n",
    "    plt.plot(reduce_loss(history_shadow.history[\"val_loss\"]), label=\"shadow model\")\n",
    "plt.plot(reduce_loss(history_target.history[\"val_loss\"]), label=\"target model\")\n",
    "plt.legend()\n",
    "plt.title(\"LOSS (val)\")\n",
    "plt.savefig(pattern_graphs_loss)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_shadow:\n",
    "    plt.plot(history_shadow.history[\"categorical_accuracy\"], label=\"shadow model\")\n",
    "plt.plot(history_target.history[\"categorical_accuracy\"], label=\"target model\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epcohs\")\n",
    "plt.title(\"ACCURACY (VAL)\")\n",
    "plt.savefig(pattern_graphs_accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up the attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a dataset $D^*$ to train the attack__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model on the \"in\" part of the data, we can make a prediction on both dataset's parts (\"in\" and \"out\") a labelise the results. The new dataset is named $D*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_pred(x):\n",
    "    ds_x = tf.data.Dataset.from_tensor_slices(x)\\\n",
    "                                  .map(reshape_images)\\\n",
    "                                  .batch(x.shape[0])\n",
    "    return ds_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star_in = model_shadow.predict(x_prim_in)\n",
    "y_star_in = [1 for i in range(len(x_star_in))]\n",
    "\n",
    "x_star_out = model_shadow.predict(x_prim_out)\n",
    "y_star_out = [0 for i in range(len(x_star_out))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.concatenate([x_star_in, x_star_out], axis=0)\n",
    "y_star = np.concatenate([y_star_in, y_star_out], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_star_train, \\\n",
    "x_star_test, \\\n",
    "y_star_train, \\\n",
    "y_star_test = train_test_split(x_star, y_star, test_size =.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create XGBOOST attack model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref : https://www.datacamp.com/community/tutorials/xgboost-in-python#apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_attack  = xgb.XGBClassifier(objective ='reg:squarederror',\n",
    "                                colsample_bytree = 0.8,\n",
    "                                learning_rate = 0.01,\n",
    "                                max_depth = 5,\n",
    "                                alpha = 10,\n",
    "                                n_estimators = 20)\n",
    "\n",
    "clf_attack.fit(x_star_train, y_star_train)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_star_test, clf_attack.predict(x_star_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_attack  = xgb.XGBClassifier(objective ='reg:squarederror',\n",
    "                                colsample_bytree = 0.3,\n",
    "                                learning_rate = 0.1,\n",
    "                                max_depth = 5,\n",
    "                                alpha = 10,\n",
    "                                n_estimators = 20)\n",
    "clf_attack.fit(x_star,y_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the attack against the true data set D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(target_model, attack_model, data, label):\n",
    "    \n",
    "    # Information we have thanks to the API (original model)\n",
    "    probas   = target_model.predict(data)\n",
    "\n",
    "    # Model we have trained to make the attack\n",
    "    prediction = clf_attack.predict(probas)\n",
    "\n",
    "    # Results zipping prediction an true labels\n",
    "    result  = pd.DataFrame(zip(prediction, [label for i in range(len(probas))]), \n",
    "                           columns = (\"y_pred\", \"y\"))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for images out of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_a = get_predictions_and_labels(target_model = target_model, \n",
    "                                       attack_model=clf_attack, \n",
    "                                       data=x_a, label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for images in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_b = get_predictions_and_labels(target_model = target_model, \n",
    "                                       attack_model=clf_attack,\n",
    "                                       data=x_b, label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy of the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack_results = pd.concat([results_a, results_b]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(attack_results[\"y\"], attack_results[\"y_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {\n",
    "    \"dataset\" : [\"cifar10\"],\n",
    "    \"attack_model\" : [\"XGBoost\"],\n",
    "    \"accuracy_target\" : [target_model.evaluate(x_a[:1000], y_a[:1000], batch_size=1000, verbose=0)[1]],\n",
    "    \"accuracy_shadow\" : [model_shadow.evaluate(x_prim_out[:1000], y_prim_out[:1000], batch_size=1000, verbose=0)[1]],\n",
    "    \"accurracy_attack\" : [metrics.accuracy_score(attack_results[\"y\"], attack_results[\"y_pred\"])],\n",
    "    \"DP_multiplicator\" : [noise_multiplier],\n",
    "    \"droupout\" : [dropout],\n",
    "    \"learning_rate\" : [learning_rate],\n",
    "    \"num_epochs\" : [epochs]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd = pd.DataFrame(result_dict)\n",
    "result_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"results.csv\" in os.listdir(\"models\"):\n",
    "    main_pd = pd.read_csv(\"models/results.csv\", index_col=0)\n",
    "    result_pd = pd.concat([main_pd, result_pd], ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go = input(\"do you want to save the results ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if go.lower().startswith(\"y\"):\n",
    "    print(\"Results saved\")\n",
    "    result_pd.to_csv(\"models/results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve and interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pd = pd.read_csv(\"models/results.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pd"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf22-gpu.2-2.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf22-gpu.2-2:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
