{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership inference attack with images\n",
    "## Target a CNN\n",
    "Authors : Johan Jublanc\n",
    "\n",
    "We use this article to simulate a membership inference attack : https://arxiv.org/pdf/1807.09173.pdf\n",
    "\n",
    "Usefull reference : https://medium.com/disaitek/demystifying-the-membership-inference-attack-e33e510a0c39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import tarfile\n",
    "\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from dp_optimizer_fn import make_gaussian_optimizer_class\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.__version__ == 2.x\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data from cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the CIFAR10 data which is a dataset of color images of size 32x32. For more information let's go here :\n",
    "- https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "CIFAR10 data are splited in batches. For this example the first batche is used to build up a classifier and the second one will be used to build up the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "    data_dir = tf.keras.utils.get_file(origin=url, fname='cifar10', untar=True)\n",
    "\n",
    "    root_keras_data_path = \"/\".join(data_dir.split(\"/\")[:5])\n",
    "    cifar_data_path = os.path.join(root_keras_data_path, \"cifar-10-batches-py\")\n",
    "\n",
    "    data_batches_names = []\n",
    "    for item in os.listdir(cifar_data_path):\n",
    "        if item.startswith(\"data_batch\"):\n",
    "            data_batches_names.append(item)\n",
    "    \n",
    "    print(\"Files used to build the data list : \")\n",
    "    \n",
    "    data = []\n",
    "    for data_batches_name in data_batches_names:\n",
    "        data.append(unpickle(os.path.join(cifar_data_path, data_batches_name)))\n",
    "        print(data_batches_name)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly build a model that is trained on the dataset $data_b$, the dataset $data_a$ is used to evaluate the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = data[0][b\"data\"]\n",
    "y_a = data[0][b\"labels\"]\n",
    "\n",
    "x_b = data[1][b\"data\"]\n",
    "y_b = data[1][b\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get a shadow dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the attacker knows another dataset that is similar to D. Here we use batch 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prim_in = data[2][b\"data\"]\n",
    "y_prim_in = data[2][b\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Batch 3 is used to get intput out of scope used to train the shadow model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prim_out = data[3][b'data']\n",
    "y_prim_out = data[3][b'labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define training parameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "SHUFFLE_SIZE = 200\n",
    "NUM_EPOCHS = 1 #300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "num_classes = 10\n",
    "len_train = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = True\n",
    "train_shadow = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = False\n",
    "noise_multiplier = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name_patterns(dropout, noise_multiplier):\n",
    "    pattern_target = \"models/target\" + dropout*\"WithDropOut\" + \"DP\" + str(noise_multiplier) + \"_*.h5\"\n",
    "    pattern_shadow = \"models/shadow\" + dropout*\"WithDropOut\" + \"_*.h5\"\n",
    "    print(pattern_target)\n",
    "    print(pattern_shadow)\n",
    "    return pattern_target, pattern_shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_target, pattern_shadow = get_model_name_patterns(dropout, noise_multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function constituting the whole pipeline :\n",
    "* reshape images\n",
    "* create the decay callback\n",
    "* create a MapDataset\n",
    "* plt example images\n",
    "* create a target\n",
    "* load and save trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_images(flat_array):\n",
    "    flat_array_normalized = tf.cast(flat_array, tf.float32) / 255.\n",
    "    img_reshaped = tf.reshape(flat_array_normalized, (3, 32, 32))\n",
    "    return tf.transpose(img_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    cycle = epoch // 50\n",
    "    \n",
    "    if cycle == 0:\n",
    "        return 0.001\n",
    "    elif cycle <= 2:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dp_optimizer_fn import LearningRateScheduler_Perso\n",
    "callback = LearningRateScheduler_Perso(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to create a dataset tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(flat_arrays, labels, \n",
    "             BATCH_SIZE = BATCH_SIZE, \n",
    "             SHUFFLE_SIZE = SHUFFLE_SIZE, \n",
    "             NUM_EPOCHS = NUM_EPOCHS):\n",
    "    ds_x = tf.data.Dataset.from_tensor_slices(flat_arrays)\n",
    "    ds_x = ds_x.map(reshape_images)\n",
    "    ds_y = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    ds_x_y = tf.data\\\n",
    "               .Dataset\\\n",
    "               .zip((ds_x, ds_y))\\\n",
    "               .shuffle(SHUFFLE_SIZE)\\\n",
    "               .repeat()\\\n",
    "               .batch(BATCH_SIZE)\\\n",
    "               .prefetch(1)\n",
    "    return ds_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_labels(img_batch, label_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        img = rotate(img_batch[n], -90)\n",
    "        plt.imshow(img)\n",
    "        plt.title(str(label_batch[n].numpy()))\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first model is trained on 80% of the $data_b$ and test on the 20% left\n",
    "We use this article to build a quite good model : https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, dropout=dropout):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), \n",
    "                     activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    if dropout :\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    if dropout :\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    if dropout :\n",
    "        model.add(Dropout(0.2))\n",
    "        \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', \n",
    "                    kernel_initializer='he_uniform'))\n",
    "    if dropout :\n",
    "        model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load/Save model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"models\" not in os.listdir():\n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_model_num(pattern):\n",
    "    target_models_list = glob.glob(pattern)\n",
    "    num_list = [x.split(\"/\")[1].split(\".\")[0].split(\"_\")[1] for x in target_models_list]\n",
    "    num_list_int = [int(x) for x in num_list]\n",
    "    return np.max(num_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_model(pattern, model):\n",
    "    if len(glob.glob(pattern))==0:\n",
    "        model.save(pattern.split(\"_\")[0] + \"_0.h5\")\n",
    "    else:\n",
    "        num = return_model_num(pattern) + 1\n",
    "        model.save(pattern.replace(\"*\", str(num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(pattern, input_shape, noise_multiplier=0, dropout=dropout):\n",
    "    \n",
    "    ##################################################\n",
    "    # create or retrieve model structure and weights #\n",
    "    ##################################################\n",
    "    \n",
    "    model = create_model(input_shape, dropout=dropout)\n",
    "    \n",
    "    #if len(glob.glob(pattern))==0:\n",
    "    #    model = create_model(input_shape, dropout=dropout, learning_rate=learning_rate)\n",
    "    #    print(\"\\nModel created from scratch\")\n",
    "    #else:\n",
    "    #    num = return_model_num(pattern)\n",
    "    #    filepath = pattern.replace(\"*\", str(num))\n",
    "    #    model = tf.keras.models.load_model(filepath)\n",
    "    #   print(\"\\nModel retrieved from the file : {}\\n\".format(filepath))\n",
    "    \n",
    "    ####################\n",
    "    # get the optimizer#\n",
    "    ####################\n",
    "    if noise_multiplier > 0 :\n",
    "        # Use differentrial privacy\n",
    "        GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
    "        DPGradientDescentGaussianOptimizer = make_gaussian_optimizer_class(GradientDescentOptimizer)\n",
    "        noise_multiplier = 2\n",
    "        optimizer = DPGradientDescentGaussianOptimizer(l2_norm_clip=1,\n",
    "                                                       noise_multiplier=noise_multiplier,\n",
    "                                                       learning_rate=0.005)\n",
    "    else:\n",
    "        optimizer = SGD(lr=0.005)\n",
    "    \n",
    "    ####################\n",
    "    # compile the model#\n",
    "    ####################\n",
    "    print(\"Manual compilation of the model\")\n",
    "    print(\"Optimizer : {}\".format(optimizer))\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train target model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create dataset objects__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_xy_b = input_fn(x_b, y_b)\n",
    "ds_xy_a = input_fn(x_a, y_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot some example__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(y_b)\n",
    "img_batch, label_batch = next(iter(ds_xy_b))\n",
    "plt_img_labels(img_batch, label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load pretrained or create a new model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_target_model(noise_multiplier):\n",
    "    target_model = get_model(pattern_target, input_shape, noise_multiplier=noise_multiplier)\n",
    "    score = target_model.evaluate(ds_xy_a, steps=200, verbose=0)\n",
    "\n",
    "    print(\"\\n\\nPERFORMANCES BEFORE TRAINNING: \")\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    history=None\n",
    "    if train_target:\n",
    "        history = target_model.fit(ds_xy_b,\n",
    "                                    epochs=NUM_EPOCHS,\n",
    "                                    steps_per_epoch=len_train//BATCH_SIZE,\n",
    "                                    verbose=1,\n",
    "                                    validation_data=(ds_xy_a),\n",
    "                                    validation_steps=200,\n",
    "                                    callbacks=[callback])\n",
    "        score = target_model.evaluate(ds_xy_a, steps=200, verbose=0)\n",
    "    \n",
    "        print(\"\\n\\nPERFORMANCES AFTER TRAINNING: \")\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "        print(\"\\n\")\n",
    "\n",
    "        save_new_model(pattern_target, target_model)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = train_and_save_target_model(noise_multiplier=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"lr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Retrieve the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = get_model(pattern_target, input_shape, noise_multiplier=noise_multiplier)\n",
    "target_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the shadow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_prim_in = input_fn(x_prim_in, y_prim_in)\n",
    "ds_prim_out = input_fn(x_prim_out, y_prim_out)\n",
    "\n",
    "img_batch, label_batch = next(iter(ds_prim_in))\n",
    "plt_img_labels(img_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_shadow_model(learning_rate):\n",
    "    model_shadow = get_model(pattern_shadow, input_shape, learning_rate=learning_rate)\n",
    "    score = model_shadow.evaluate(ds_prim_out, steps=200, verbose=0)\n",
    "\n",
    "    print(\"\\n\\nPERFORMANCES BEFORE TRAINING: \")\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    \n",
    "    history_shadow = None\n",
    "    if train_shadow:\n",
    "        history_shadow = model_shadow.fit(ds_prim_in,\n",
    "                                      epochs=NUM_EPOCHS,\n",
    "                                      steps_per_epoch=len_train//BATCH_SIZE,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=(ds_prim_out),\n",
    "                                      validation_steps=200,\n",
    "                                      callbacks=[callback])\n",
    "        score = model_shadow.evaluate(ds_prim_out, steps=200, verbose=0)\n",
    "        print(\"\\n\\nPERFORMANCES AFTER TRAINING: \")\n",
    "        print('Test loss:', score[0])\n",
    "        print('Test accuracy:', score[1])\n",
    "        \n",
    "        save_new_model(pattern_shadow, model_shadow)\n",
    "    return history_shadow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_shadow = train_and_save_shadow_model(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shadow = get_model(pattern_shadow, input_shape)\n",
    "model_shadow.summary()\n",
    "score = model_shadow.evaluate(ds_prim_out, steps=200, verbose=0)\n",
    "\n",
    "print(\"\\n\\nPERFORMANCES BEFORE TRAINING: \")\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up the attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a dataset $D^*$ to train the attack__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model on the \"in\" part of the data, we can make a prediction on both dataset's parts (\"in\" and \"out\") a labelise the results. The new dataset is named $D*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_pred(x):\n",
    "    ds_x = tf.data.Dataset.from_tensor_slices(x)\\\n",
    "                                  .map(reshape_images)\\\n",
    "                                  .batch(x.shape[0])\n",
    "    return ds_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_x_prim_in = input_fn_pred(x_prim_in)\n",
    "ds_x_prim_out = input_fn_pred(x_prim_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star_in = model_shadow.predict(ds_x_prim_in)\n",
    "y_star_in = [1 for i in range(len(x_star_in))]\n",
    "\n",
    "x_star_out = model_shadow.predict(ds_x_prim_out)\n",
    "y_star_out = [0 for i in range(len(x_star_out))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.concatenate([x_star_in, x_star_out], axis=0)\n",
    "y_star = np.concatenate([y_star_in, y_star_out], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_star_train, \\\n",
    "x_star_test, \\\n",
    "y_star_train, \\\n",
    "y_star_test = train_test_split(x_star, y_star, test_size =.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create XGBOOST attack model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref : https://www.datacamp.com/community/tutorials/xgboost-in-python#apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_attack  = xgb.XGBClassifier(objective ='reg:squarederror',\n",
    "                                colsample_bytree = 0.8,\n",
    "                                learning_rate = 0.01,\n",
    "                                max_depth = 5,\n",
    "                                alpha = 10,\n",
    "                                n_estimators = 20)\n",
    "\n",
    "clf_attack.fit(x_star_train, y_star_train)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_star_test, clf_attack.predict(x_star_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_attack  = xgb.XGBClassifier(objective ='reg:squarederror',\n",
    "                                colsample_bytree = 0.3,\n",
    "                                learning_rate = 0.1,\n",
    "                                max_depth = 5,\n",
    "                                alpha = 10,\n",
    "                                n_estimators = 10)\n",
    "clf_attack.fit(x_star,y_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the attack against the true data set D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(target_model, attack_model, data, label):\n",
    "    \n",
    "    ds_data = input_fn_pred(data)\n",
    "    \n",
    "    # Information we have thanks to the API (original model)\n",
    "    probas   = target_model.predict(ds_data)\n",
    "\n",
    "    # Model we have trained to make the attack\n",
    "    prediction = clf_attack.predict(probas)\n",
    "\n",
    "    # Results zipping prediction an true labels\n",
    "    result  = pd.DataFrame(zip(prediction, [label for i in range(len(probas))]), \n",
    "                           columns = (\"y_pred\", \"y\"))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for images out of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_a = get_predictions_and_labels(target_model = target_model, \n",
    "                                       attack_model=clf_attack, \n",
    "                                       data=x_a, label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for images in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_b = get_predictions_and_labels(target_model = target_model, \n",
    "                                       attack_model=clf_attack,\n",
    "                                       data=x_b, label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy of the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack_results = pd.concat([results_a, results_b]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(attack_results[\"y\"], attack_results[\"y_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"cifar10\"\n",
    "attack_model = \"XGBoost\"\n",
    "accuracy_target = target_model.evaluate(ds_xy_a, steps=200, verbose=0)[1]\n",
    "accuracy_shadow = model_shadow.evaluate(ds_prim_out, steps=200, verbose=0)[1]\n",
    "accurracy_attack = metrics.accuracy_score(attack_results[\"y\"], attack_results[\"y_pred\"])\n",
    "DP_multiplicator = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {\n",
    "    \"dataset\" : [\"cifar10\"],\n",
    "    \"attack_model\" : [\"XGBoost\"],\n",
    "    \"accuracy_target\" : [target_model.evaluate(ds_xy_a, steps=200, verbose=0)[1]],\n",
    "    \"accuracy_shadow\" : [model_shadow.evaluate(ds_prim_out, steps=200, verbose=0)[1]],\n",
    "    \"accurracy_attack\" : [metrics.accuracy_score(attack_results[\"y\"], attack_results[\"y_pred\"])],\n",
    "    \"DP_multiplicator\" : [noise_multiplier],\n",
    "    \"droupout\" : [dropout],\n",
    "    \"num_epochs\" : [NUM_EPOCHS]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd = pd.DataFrame(result_dict)\n",
    "result_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : trouver un moyen de sauvegarder les learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if \"results.csv\" in os.listdir(\"models\"):\n",
    "#    main_pd = pd.read_csv(\"models/results.csv\", index_col=0)\n",
    "#    result_pd = pd.concat([main_pd, result_pd], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_pd.to_csv(\"models/results.csv\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf22-gpu.2-2.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf22-gpu.2-2:m47"
  },
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
