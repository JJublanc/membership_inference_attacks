{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership inference attack with images\n",
    "## Target a CNN\n",
    "Authors : Johan Jublanc\n",
    "\n",
    "We use this article to simulate a membership inference attack : https://arxiv.org/pdf/1807.09173.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import tarfile\n",
    "\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tf.__version__ == 2.x\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data from cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the CIFAR10 data which is a dataset of color images of size 32x32. For more information let's go here :\n",
    "- https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process can take a while ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "# urllib.request.urlretrieve(url, data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = tf.keras.utils.get_file(origin=url, fname='cifar10', untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_keras_data_path = \"/\".join(data_dir.split(\"/\")[:5])\n",
    "cifar_data_path = os.path.join(root_keras_data_path, \"cifar-10-batches-py\")\n",
    "os.listdir(cifar_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batches_names = []\n",
    "for item in os.listdir(cifar_data_path):\n",
    "    if item.startswith(\"data_batch\"):\n",
    "        data_batches_names.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 data are splited in batches. For this example the first batche is used to build up a classifier and the second one will be used to build up the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for data_batches_name in data_batches_names:\n",
    "    data.append(unpickle(os.path.join(cifar_data_path, data_batches_name)))\n",
    "    print(data_batches_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly build a model that is trained on the dataset $data_b$, the dataset $data_a$ is used to evaluate the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_a = data[0][b\"data\"]\n",
    "y_a = data[0][b\"labels\"]\n",
    "x_a1, x_a2, y_a1, y_a2 = train_test_split(x_a, y_a, test_size=.2)\n",
    "\n",
    "x_b = data[1][b\"data\"]\n",
    "y_b = data[1][b\"labels\"]\n",
    "x_b1, x_b2, y_b1, y_b2 = train_test_split(x_b, y_b, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get a shadow dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the attacker knows another dataset that is similar to D. Here we use batch 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prim_in = data[2][b\"data\"]\n",
    "y_prim_in = data[2][b\"labels\"]\n",
    "print(os.path.join(cifar_data_path, data_batches_names[0]))\n",
    "\n",
    "x_prim_in_train, \\\n",
    "x_prim_in_test, \\\n",
    "y_prim_in_train, \\\n",
    "y_prim_in_test = train_test_split(x_prim_in,\n",
    "                                  y_prim_in,\n",
    "                                  test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Batch 3 is used to get intput out of scope used to train the shadow model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prim_out = data[3][b'data']\n",
    "y_prim_out = data[3][b'labels']\n",
    "\n",
    "x_prim_out_train, \\\n",
    "x_prim_out_test, \\\n",
    "y_prim_out_train, \\\n",
    "y_prim_out_test = train_test_split(x_prim_out,\n",
    "                                   y_prim_out,\n",
    "                                   test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define training parameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "SHUFFLE_SIZE = 200\n",
    "NUM_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "num_classes = 10\n",
    "len_train = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function constituting the whole pipeline :\n",
    "* reshape images\n",
    "* create a MapDataset\n",
    "* plt example images\n",
    "* create a target\n",
    "* load and save trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_images(flat_array):\n",
    "    flat_array_normalized = tf.cast(flat_array, tf.float32) / 255.\n",
    "    img_reshaped = tf.reshape(flat_array_normalized, (3, 32, 32))\n",
    "    return tf.transpose(img_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to create a dataset tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(flat_arrays, labels, \n",
    "             BATCH_SIZE = BATCH_SIZE, \n",
    "             SHUFFLE_SIZE = SHUFFLE_SIZE, \n",
    "             NUM_EPOCHS = NUM_EPOCHS):\n",
    "    ds_x = tf.data.Dataset.from_tensor_slices(flat_arrays)\n",
    "    ds_x = ds_x.map(reshape_images)\n",
    "    ds_y = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    ds_x_y = tf.data\\\n",
    "               .Dataset\\\n",
    "               .zip((ds_x, ds_y))\\\n",
    "               .shuffle(SHUFFLE_SIZE)\\\n",
    "               .repeat(NUM_EPOCHS)\\\n",
    "               .batch(BATCH_SIZE)\\\n",
    "               .prefetch(1)\n",
    "    return ds_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_labels(img_batch, label_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        img = rotate(img_batch[n], -90)\n",
    "        plt.imshow(img)\n",
    "        plt.title(str(label_batch[n].numpy()))\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first model is trained on 80% of the $data_b$ and test on the 20% left\n",
    "We use this article to build a quite good model : https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), \n",
    "                     activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', \n",
    "                    kernel_initializer='he_uniform'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=opt,\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load/Save model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"models\" not in os.listdir():\n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def return_model_num(pattern):\n",
    "    target_models_list = glob.glob(pattern)\n",
    "    num_list = [x.split(\"/\")[1].split(\".\")[0].split(\"_\")[1] for x in target_models_list]\n",
    "    num_list_int = [int(x) for x in num_list]\n",
    "    return np.max(num_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_model(pattern, model):\n",
    "    if len(glob.glob(pattern))==0:\n",
    "        model.save(pattern.split(\"_\")[0] + \"_0.h5\")\n",
    "    else:\n",
    "        num = return_model_num(pattern) + 1\n",
    "        model.save(pattern.replace(\"*\", str(num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(pattern, input_shape):\n",
    "    if len(glob.glob(pattern))==0:\n",
    "        model = create_model(input_shape)\n",
    "    else:\n",
    "        num = return_model_num(pattern)\n",
    "        filepath = pattern.replace(\"*\", str(num))\n",
    "        model = tf.keras.models.load_model(filepath)\n",
    "        print(filepath)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train target model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create dataset objects__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_xy_b1 = input_fn(x_b1, y_b1)\n",
    "ds_xy_b2 = input_fn(x_b2, y_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot some example__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(y_b1)\n",
    "img_batch, label_batch = next(iter(ds_xy_b1))\n",
    "plt_img_labels(img_batch, label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load pretrained or create a new model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'models/target_*.h5'\n",
    "target_model = get_model(pattern, input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = target_model.fit(ds_xy_b1,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            steps_per_epoch=len_train//BATCH_SIZE,\n",
    "            verbose=1,\n",
    "            validation_data=(ds_xy_b2))\n",
    "score = target_model.evaluate(ds_xy_b2, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_new_model(pattern, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'models/target_*.h5'\n",
    "target_model = get_model(pattern, input_shape)\n",
    "target_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the shadow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_prim_in_train = input_fn(x_prim_in_train, y_prim_in_train)\n",
    "ds_prim_in_test = input_fn(x_prim_in_test, y_prim_in_test)\n",
    "\n",
    "img_batch, label_batch = next(iter(ds_prim_in_train))\n",
    "plt_img_labels(img_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'models/prim_*.h5'\n",
    "model_shadow = get_model(pattern,input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_shadow = model_shadow.fit(ds_prim_in_train,\n",
    "                              epochs=NUM_EPOCHS,\n",
    "                              steps_per_epoch=len_train//BATCH_SIZE,\n",
    "                              verbose=1,\n",
    "                              validation_data=(ds_prim_in_test))\n",
    "score = model_shadow.evaluate(ds_prim_in_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_new_model(pattern, model_shadow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up the attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a dataset $D^*$ to train the attack__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model on the \"in\" part of the data, we can make a prediction on both dataset's parts (\"in\" and \"out\") a labelise the results. The new dataset is named $D*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_pred(x):\n",
    "    ds_x = tf.data.Dataset.from_tensor_slices(x)\\\n",
    "                                  .map(reshape_images)\\\n",
    "                                  .batch(x.shape[0])\n",
    "    return ds_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_x_prim_in = input_fn_pred(x_prim_in_train)\n",
    "ds_x_prim_out = input_fn_pred(x_prim_out_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star_in = model_shadow.predict(ds_x_prim_in)\n",
    "y_star_in = [1 for i in range(len(x_star_in))]\n",
    "\n",
    "x_star_out = model_shadow.predict(ds_x_prim_out)\n",
    "y_star_out = [0 for i in range(len(x_star_out))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.concatenate([x_star_in, x_star_out], axis=0)\n",
    "y_star = np.concatenate([y_star_in, y_star_out], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star_train, \\\n",
    "x_star_test, \\\n",
    "y_star_train, \\\n",
    "y_star_test = train_test_split(x_star, y_star, test_size =.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create XGBOOST attack model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref : https://www.datacamp.com/community/tutorials/xgboost-in-python#apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_attack  = xgb.XGBClassifier(objective ='reg:squarederror',\n",
    "                                colsample_bytree = 0.8,\n",
    "                                learning_rate = 0.01,\n",
    "                                max_depth = 5,\n",
    "                                alpha = 10,\n",
    "                                n_estimators = 20)\n",
    "\n",
    "clf_attack.fit(x_star_train, y_star_train)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_star_test, clf_attack.predict(x_star_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_attack  = xgb.XGBClassifier(objective ='reg:squarederror',\n",
    "                                colsample_bytree = 0.3,\n",
    "                                learning_rate = 0.1,\n",
    "                                max_depth = 5,\n",
    "                                alpha = 10,\n",
    "                                n_estimators = 10)\n",
    "clf_attack.fit(x_star,y_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create NN attack model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_SIZE = 1000\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_star_train_x = tf.data.Dataset.from_tensor_slices(x_star_train)\n",
    "ds_star_train_y = tf.data.Dataset.from_tensor_slices(y_star_train)\n",
    "ds_star_train_xy = tf.data.Dataset\\\n",
    "                          .zip((ds_star_train_x, ds_star_train_y))\\\n",
    "                          .shuffle(SHUFFLE_SIZE)\\\n",
    "                          .repeat(NUM_EPOCHS)\\\n",
    "                          .batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_star_test_x = tf.data.Dataset.from_tensor_slices(x_star_test)\n",
    "ds_star_test_y = tf.data.Dataset.from_tensor_slices(y_star_test)\n",
    "ds_star_test_xy = tf.data.Dataset\\\n",
    "                          .zip((ds_star_test_x, ds_star_test_y))\\\n",
    "                          .shuffle(SHUFFLE_SIZE)\\\n",
    "                          .repeat(NUM_EPOCHS)\\\n",
    "                          .batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_attack = Sequential()\n",
    "NN_attack.add(Dense(50, activation='relu', input_shape = (10,)))\n",
    "NN_attack.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "NN_attack.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_attack = NN_attack.fit(ds_star_train_xy,\n",
    "                               epochs=NUM_EPOCHS,\n",
    "                               steps_per_epoch=8000//BATCH_SIZE,\n",
    "                               validation_data=ds_star_test_xy\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the attack against the true data set D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(target_model, attack_model, data, label):\n",
    "    \n",
    "    ds_data = input_fn_pred(data)\n",
    "    \n",
    "    # Information we have thanks to the API (original model)\n",
    "    probas   = target_model.predict(ds_data)\n",
    "\n",
    "    # Model we have trained to make the attack\n",
    "    prediction = clf_attack.predict(probas)\n",
    "\n",
    "    # Results zipping prediction an true labels\n",
    "    result  = pd.DataFrame(zip(prediction, [label for i in range(len(probas))]), \n",
    "                           columns = (\"y_pred\", \"y\"))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for images out of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_a = get_predictions_and_labels(target_model = target_model, \n",
    "                                       attack_model=NN_attack, \n",
    "                                       data=x_a1, label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for images in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_b = get_predictions_and_labels(target_model = target_model, \n",
    "                                       attack_model=NN_attack,\n",
    "                                       data=x_b1, label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy of the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack_results = pd.concat([results_a, results_b]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(attack_results[\"y\"], attack_results[\"y_pred\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
