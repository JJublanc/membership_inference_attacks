{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membership inference attack with images\n",
    "## Target a CNN\n",
    "Authors : Johan Jublanc\n",
    "\n",
    "We use this article to simulate a membership inference attack : https://arxiv.org/pdf/1807.09173.pdf\n",
    "\n",
    "Usefull reference : https://medium.com/disaitek/demystifying-the-membership-inference-attack-e33e510a0c39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import tarfile\n",
    "\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.__version__ == 2.x\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data from cifar10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the CIFAR10 data which is a dataset of color images of size 32x32. For more information let's go here :\n",
    "- https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process can take a while ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n",
    "# urllib.request.urlretrieve(url, data_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = tf.keras.utils.get_file(origin=url, fname='cifar10', untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_keras_data_path = \"/\".join(data_dir.split(\"/\")[:5])\n",
    "cifar_data_path = os.path.join(root_keras_data_path, \"cifar-10-batches-py\")\n",
    "os.listdir(cifar_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batches_names = []\n",
    "for item in os.listdir(cifar_data_path):\n",
    "    if item.startswith(\"data_batch\"):\n",
    "        data_batches_names.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 data are splited in batches. For this example the first batche is used to build up a classifier and the second one will be used to build up the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for data_batches_name in data_batches_names:\n",
    "    data.append(unpickle(os.path.join(cifar_data_path, data_batches_name)))\n",
    "    print(data_batches_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Split data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We firstly build a model that is trained on the dataset $data_b$, the dataset $data_a$ is used to evaluate the attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_a = data[0][b\"data\"]\n",
    "y_a = data[0][b\"labels\"]\n",
    "\n",
    "x_b = data[1][b\"data\"]\n",
    "y_b = data[1][b\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Get a shadow dataset__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the attacker knows another dataset that is similar to D. Here we use batch 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prim_in = data[2][b\"data\"]\n",
    "y_prim_in = data[2][b\"labels\"]\n",
    "print(os.path.join(cifar_data_path, data_batches_names[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Batch 3 is used to get intput out of scope used to train the shadow model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_prim_out = data[3][b'data']\n",
    "y_prim_out = data[3][b'labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define training parameters__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "SHUFFLE_SIZE = 200\n",
    "NUM_EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "num_classes = 10\n",
    "len_train = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = True\n",
    "train_shadow = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function constituting the whole pipeline :\n",
    "* reshape images\n",
    "* create a MapDataset\n",
    "* plt example images\n",
    "* create a target\n",
    "* load and save trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_images(flat_array):\n",
    "    flat_array_normalized = tf.cast(flat_array, tf.float32) / 255.\n",
    "    img_reshaped = tf.reshape(flat_array_normalized, (3, 32, 32))\n",
    "    return tf.transpose(img_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to create a dataset tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(flat_arrays, labels, \n",
    "             BATCH_SIZE = BATCH_SIZE, \n",
    "             SHUFFLE_SIZE = SHUFFLE_SIZE, \n",
    "             NUM_EPOCHS = NUM_EPOCHS):\n",
    "    ds_x = tf.data.Dataset.from_tensor_slices(flat_arrays)\n",
    "    ds_x = ds_x.map(reshape_images)\n",
    "    ds_y = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    ds_x_y = tf.data\\\n",
    "               .Dataset\\\n",
    "               .zip((ds_x, ds_y))\\\n",
    "               .shuffle(SHUFFLE_SIZE)\\\n",
    "               .repeat()\\\n",
    "               .batch(BATCH_SIZE)\\\n",
    "               .prefetch(1)\n",
    "    return ds_x_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_img_labels(img_batch, label_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "\n",
    "    for n in range(25):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        img = rotate(img_batch[n], -90)\n",
    "        plt.imshow(img)\n",
    "        plt.title(str(label_batch[n].numpy()))\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first model is trained on 80% of the $data_b$ and test on the 20% left\n",
    "We use this article to build a quite good model : https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, opt):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), \n",
    "                     activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same', \n",
    "                     input_shape=input_shape))\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', \n",
    "                     kernel_initializer='he_uniform', \n",
    "                     padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', \n",
    "                    kernel_initializer='he_uniform'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=opt,\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load/Save model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"models\" not in os.listdir():\n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def return_model_num(pattern):\n",
    "    target_models_list = glob.glob(pattern)\n",
    "    num_list = [x.split(\"/\")[1].split(\".\")[0].split(\"_\")[1] for x in target_models_list]\n",
    "    num_list_int = [int(x) for x in num_list]\n",
    "    return np.max(num_list_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_model(pattern, model):\n",
    "    if len(glob.glob(pattern))==0:\n",
    "        model.save(pattern.split(\"_\")[0] + \"_0.h5\")\n",
    "    else:\n",
    "        num = return_model_num(pattern) + 1\n",
    "        model.save(pattern.replace(\"*\", str(num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(pattern, input_shape, opt):\n",
    "    if len(glob.glob(pattern))==0:\n",
    "        model = create_model(input_shape, opt)\n",
    "    else:\n",
    "        num = return_model_num(pattern)\n",
    "        filepath = pattern.replace(\"*\", str(num))\n",
    "        model = tf.keras.models.load_model(filepath)\n",
    "        print(filepath)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train target model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create dataset objects__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_xy_b = input_fn(x_b, y_b)\n",
    "ds_xy_a = input_fn(x_a, y_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Plot some example__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = len(y_b)\n",
    "img_batch, label_batch = next(iter(ds_xy_b))\n",
    "plt_img_labels(img_batch, label_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load pretrained or create a new model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging\n",
    "import collections\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import privacy_ledger\n",
    "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
    "\n",
    "def make_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP optimizer class from an existing one.\"\"\"\n",
    "  parent_code = tf.compat.v1.train.Optimizer.compute_gradients.__code__\n",
    "  child_code = cls.compute_gradients.__code__\n",
    "  GATE_OP = tf.compat.v1.train.Optimizer.GATE_OP  # pylint: disable=invalid-name\n",
    "  if child_code is not parent_code:\n",
    "    logging.warning(\n",
    "        'WARNING: Calling make_optimizer_class() on class %s that overrides '\n",
    "        'method compute_gradients(). Check to ensure that '\n",
    "        'make_optimizer_class() does not interfere with overridden version.',\n",
    "        cls.__name__)\n",
    "\n",
    "  class DPOptimizerClass(cls):\n",
    "    \"\"\"Differentially private subclass of given class cls.\"\"\"\n",
    "\n",
    "    _GlobalState = collections.namedtuple(\n",
    "      '_GlobalState', ['l2_norm_clip', 'stddev'])\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dp_sum_query,\n",
    "        num_microbatches=None,\n",
    "        unroll_microbatches=False,\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
    "        **kwargs):\n",
    "      \"\"\"Initialize the DPOptimizerClass.\n",
    "\n",
    "      Args:\n",
    "        dp_sum_query: DPQuery object, specifying differential privacy\n",
    "          mechanism to use.\n",
    "        num_microbatches: How many microbatches into which the minibatch is\n",
    "          split. If None, will default to the size of the minibatch, and\n",
    "          per-example gradients will be computed.\n",
    "        unroll_microbatches: If true, processes microbatches within a Python\n",
    "          loop instead of a tf.while_loop. Can be used if using a tf.while_loop\n",
    "          raises an exception.\n",
    "      \"\"\"\n",
    "      super(DPOptimizerClass, self).__init__(*args, **kwargs)\n",
    "      self._dp_sum_query = dp_sum_query\n",
    "      self._num_microbatches = num_microbatches\n",
    "      self._global_state = self._dp_sum_query.initial_global_state()\n",
    "      # TODO(b/122613513): Set unroll_microbatches=True to avoid this bug.\n",
    "      # Beware: When num_microbatches is large (>100), enabling this parameter\n",
    "      # may cause an OOM error.\n",
    "      self._unroll_microbatches = unroll_microbatches\n",
    "\n",
    "    def compute_gradients(self,\n",
    "                          loss,\n",
    "                          var_list,\n",
    "                          gate_gradients=GATE_OP,\n",
    "                          aggregation_method=None,\n",
    "                          colocate_gradients_with_ops=False,\n",
    "                          grad_loss=None,\n",
    "                          gradient_tape=None,\n",
    "                          curr_noise_mult=0,\n",
    "                          curr_norm_clip=1):\n",
    "\n",
    "      self._dp_sum_query = gaussian_query.GaussianSumQuery(curr_norm_clip, \n",
    "                                                           curr_norm_clip*curr_noise_mult)\n",
    "      self._global_state = self._dp_sum_query.make_global_state(curr_norm_clip, \n",
    "                                                                curr_norm_clip*curr_noise_mult)\n",
    "      \n",
    "\n",
    "      # TF is running in Eager mode, check we received a vanilla tape.\n",
    "      if not gradient_tape:\n",
    "        raise ValueError('When in Eager mode, a tape needs to be passed.')\n",
    "\n",
    "      vector_loss = loss()\n",
    "      if self._num_microbatches is None:\n",
    "        self._num_microbatches = tf.shape(input=vector_loss)[0]\n",
    "      sample_state = self._dp_sum_query.initial_sample_state(var_list)\n",
    "      microbatches_losses = tf.reshape(vector_loss, [self._num_microbatches, -1])\n",
    "      sample_params = (self._dp_sum_query.derive_sample_params(self._global_state))\n",
    "\n",
    "      def process_microbatch(i, sample_state):\n",
    "        \"\"\"Process one microbatch (record) with privacy helper.\"\"\"\n",
    "        microbatch_loss = tf.reduce_mean(input_tensor=tf.gather(microbatches_losses, [i]))\n",
    "        grads = gradient_tape.gradient(microbatch_loss, var_list)\n",
    "        sample_state = self._dp_sum_query.accumulate_record(sample_params, sample_state, grads)\n",
    "        return sample_state\n",
    "    \n",
    "      for idx in range(self._num_microbatches):\n",
    "        sample_state = process_microbatch(idx, sample_state)\n",
    "\n",
    "      if curr_noise_mult > 0:\n",
    "        grad_sums, self._global_state = (self._dp_sum_query.get_noised_result(sample_state, self._global_state))\n",
    "      else:\n",
    "        grad_sums = sample_state\n",
    "\n",
    "      def normalize(v):\n",
    "        return v / tf.cast(self._num_microbatches, tf.float32)\n",
    "\n",
    "      final_grads = tf.nest.map_structure(normalize, grad_sums)\n",
    "      grads_and_vars = final_grads#list(zip(final_grads, var_list))\n",
    "    \n",
    "      return grads_and_vars\n",
    "\n",
    "  return DPOptimizerClass\n",
    "\n",
    "\n",
    "def make_gaussian_optimizer_class(cls):\n",
    "  \"\"\"Constructs a DP optimizer with Gaussian averaging of updates.\"\"\"\n",
    "\n",
    "  class DPGaussianOptimizerClass(make_optimizer_class(cls)):\n",
    "    \"\"\"DP subclass of given class cls using Gaussian averaging.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        l2_norm_clip,\n",
    "        noise_multiplier,\n",
    "        num_microbatches=None,\n",
    "        ledger=None,\n",
    "        unroll_microbatches=False,\n",
    "        *args,  # pylint: disable=keyword-arg-before-vararg\n",
    "        **kwargs):\n",
    "      dp_sum_query = gaussian_query.GaussianSumQuery(\n",
    "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
    "\n",
    "      if ledger:\n",
    "        dp_sum_query = privacy_ledger.QueryWithLedger(dp_sum_query,\n",
    "                                                      ledger=ledger)\n",
    "\n",
    "      super(DPGaussianOptimizerClass, self).__init__(\n",
    "          dp_sum_query,\n",
    "          num_microbatches,\n",
    "          unroll_microbatches,\n",
    "          *args,\n",
    "          **kwargs)\n",
    "\n",
    "    @property\n",
    "    def ledger(self):\n",
    "      return self._dp_sum_query.ledger\n",
    "\n",
    "  return DPGaussianOptimizerClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
    "DPGradientDescentGaussianOptimizer_NEW = make_gaussian_optimizer_class(GradientDescentOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow_privacy.privacy.optimizers import dp_optimizer\n",
    "noise_multiplier = 2\n",
    "DP_opt = DPGradientDescentGaussianOptimizer_NEW(\n",
    "      l2_norm_clip=1,\n",
    "      noise_multiplier=noise_multiplier,\n",
    "      learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'models/targetDP1_*.h5'\n",
    "target_model = get_model(pattern, input_shape, DP_opt)\n",
    "score = target_model.evaluate(ds_xy_a, steps=200, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train_target:\n",
    "    history = target_model.fit(ds_xy_b,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                steps_per_epoch=len_train//BATCH_SIZE,\n",
    "                verbose=1,\n",
    "                validation_data=(ds_xy_a),\n",
    "                validation_steps=200)\n",
    "    score = target_model.evaluate(ds_xy_a, steps=200, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_new_model(pattern, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = get_model(pattern, input_shape, DP_opt)\n",
    "target_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the shadow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_prim_in = input_fn(x_prim_in, y_prim_in)\n",
    "ds_prim_out = input_fn(x_prim_out, y_prim_out)\n",
    "\n",
    "img_batch, label_batch = next(iter(ds_prim_in))\n",
    "plt_img_labels(img_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = 'models/primDP1_*.h5'\n",
    "model_shadow = get_model(pattern,input_shape, DP_opt)\n",
    "score = model_shadow.evaluate(ds_prim_out, steps=200, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train_shadow:\n",
    "    history_shadow = model_shadow.fit(ds_prim_in,\n",
    "                                  epochs=NUM_EPOCHS,\n",
    "                                  steps_per_epoch=len_train//BATCH_SIZE,\n",
    "                                  verbose=1,\n",
    "                                  validation_data=(ds_prim_out),\n",
    "                                  validation_steps=200)\n",
    "    score = model_shadow.evaluate(ds_prim_out, steps=200, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_new_model(pattern, model_shadow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build up the attack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Build a dataset $D^*$ to train the attack__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model on the \"in\" part of the data, we can make a prediction on both dataset's parts (\"in\" and \"out\") a labelise the results. The new dataset is named $D*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_pred(x):\n",
    "    ds_x = tf.data.Dataset.from_tensor_slices(x)\\\n",
    "                                  .map(reshape_images)\\\n",
    "                                  .batch(x.shape[0])\n",
    "    return ds_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_x_prim_in = input_fn_pred(x_prim_in)\n",
    "ds_x_prim_out = input_fn_pred(x_prim_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star_in = model_shadow.predict(ds_x_prim_in)\n",
    "y_star_in = [1 for i in range(len(x_star_in))]\n",
    "\n",
    "x_star_out = model_shadow.predict(ds_x_prim_out)\n",
    "y_star_out = [0 for i in range(len(x_star_out))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.concatenate([x_star_in, x_star_out], axis=0)\n",
    "y_star = np.concatenate([y_star_in, y_star_out], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_star_train, \\\n",
    "x_star_test, \\\n",
    "y_star_train, \\\n",
    "y_star_test = train_test_split(x_star, y_star, test_size =.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create XGBOOST attack model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref : https://www.datacamp.com/community/tutorials/xgboost-in-python#apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_attack  = xgb.XGBClassifier(objective ='reg:squarederror',\n",
    "                                colsample_bytree = 0.8,\n",
    "                                learning_rate = 0.01,\n",
    "                                max_depth = 5,\n",
    "                                alpha = 10,\n",
    "                                n_estimators = 20)\n",
    "\n",
    "clf_attack.fit(x_star_train, y_star_train)\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_star_test, clf_attack.predict(x_star_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_attack  = xgb.XGBClassifier(objective ='reg:squarederror',\n",
    "                                colsample_bytree = 0.3,\n",
    "                                learning_rate = 0.1,\n",
    "                                max_depth = 5,\n",
    "                                alpha = 10,\n",
    "                                n_estimators = 10)\n",
    "clf_attack.fit(x_star,y_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the attack against the true data set D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_labels(target_model, attack_model, data, label):\n",
    "    \n",
    "    ds_data = input_fn_pred(data)\n",
    "    \n",
    "    # Information we have thanks to the API (original model)\n",
    "    probas   = target_model.predict(ds_data)\n",
    "\n",
    "    # Model we have trained to make the attack\n",
    "    prediction = clf_attack.predict(probas)\n",
    "\n",
    "    # Results zipping prediction an true labels\n",
    "    result  = pd.DataFrame(zip(prediction, [label for i in range(len(probas))]), \n",
    "                           columns = (\"y_pred\", \"y\"))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for images out of the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_a = get_predictions_and_labels(target_model = target_model, \n",
    "                                       attack_model=clf_attack, \n",
    "                                       data=x_a, label=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for images in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_b = get_predictions_and_labels(target_model = target_model, \n",
    "                                       attack_model=clf_attack,\n",
    "                                       data=x_b, label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the accuracy of the attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attack_results = pd.concat([results_a, results_b]).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(attack_results[\"y\"], attack_results[\"y_pred\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=DP_opt,\n",
    "                  metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shadow.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                     optimizer=DP_opt,\n",
    "                     metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = {\n",
    "    \"dataset\" : [\"cifar10\"],\n",
    "    \"attack_model\" : [\"XGBoost\"],\n",
    "    \"accuracy_target\" : [target_model.evaluate(ds_xy_a, steps=200, verbose=0)[1]],\n",
    "    \"accuracy_shadow\" : [model_shadow.evaluate(ds_prim_out, steps=200, verbose=0)[1]],\n",
    "    \"accurracy_attack\" : [metrics.accuracy_score(attack_results[\"y\"], attack_results[\"y_pred\"])],\n",
    "    \"DP_multiplicator\" : [noise_multiplier]\n",
    "}\n",
    "\n",
    "result_pd = pd.DataFrame(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"results.csv\" in os.listdir(\"models\"):\n",
    "    main_pd = pd.read_csv(\"models/results.csv\", index_col=0)\n",
    "    result_pd = pd.concat([main_pd, result_pd], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pd.to_csv(\"models/results.csv\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf22-gpu.2-2.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf22-gpu.2-2:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
